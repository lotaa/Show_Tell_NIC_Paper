{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1h1gqW9JLEJ4rUhA8EI_eotQlxXtrzt1u",
      "authorship_tag": "ABX9TyNIp5ngekPbQ1c33Qfn4wp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83dd2cb6c23c4b81bdbc5b5895c3396e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d36ec5f5727547ff9dbecda644c4e5c5",
              "IPY_MODEL_6fd9e75862804e76bb62a752234e0884",
              "IPY_MODEL_314ca4df6efd416cb68503858a5d8abc"
            ],
            "layout": "IPY_MODEL_15bb9ded7ae3440b8f1a652c90c5d38b"
          }
        },
        "d36ec5f5727547ff9dbecda644c4e5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3330e202d2349adafc05279591a2d5d",
            "placeholder": "​",
            "style": "IPY_MODEL_7469b06907e14c7983783a67c6afb207",
            "value": ""
          }
        },
        "6fd9e75862804e76bb62a752234e0884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_575a78b13f244d0398530d7a11ad2200",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28d4dac9e4a1496d97760c89b00a7580",
            "value": 0
          }
        },
        "314ca4df6efd416cb68503858a5d8abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3879e2bed47b4c6283d44ce16cafaf96",
            "placeholder": "​",
            "style": "IPY_MODEL_3093af2fad3c489998a407cd73a23b96",
            "value": " 0/? [03:07&lt;?, ?it/s]"
          }
        },
        "15bb9ded7ae3440b8f1a652c90c5d38b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3330e202d2349adafc05279591a2d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7469b06907e14c7983783a67c6afb207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "575a78b13f244d0398530d7a11ad2200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "28d4dac9e4a1496d97760c89b00a7580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3879e2bed47b4c6283d44ce16cafaf96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3093af2fad3c489998a407cd73a23b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lotaa/Show_Tell_NIC_Paper/blob/main/ImplementationOfPaper_ShowAndTell_NLC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETmn5dN9XTqn",
        "outputId": "98b7cc7c-b86b-4ed4-c8eb-ad5f6c864424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Flickr8k_text.zip -d /content/Flickr8k_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY9pyfoIZHoz",
        "outputId": "eac399c8-795f-4e11-e982-ac7ca889cfeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Flickr8k_text.zip\n",
            "  inflating: /content/Flickr8k_text/CrowdFlowerAnnotations.txt  \n",
            "  inflating: /content/Flickr8k_text/ExpertAnnotations.txt  \n",
            "  inflating: /content/Flickr8k_text/Flickr8k.lemma.token.txt  \n",
            "   creating: /content/Flickr8k_text/__MACOSX/\n",
            "  inflating: /content/Flickr8k_text/__MACOSX/._Flickr8k.lemma.token.txt  \n",
            "  inflating: /content/Flickr8k_text/Flickr8k.token.txt  \n",
            "  inflating: /content/Flickr8k_text/Flickr_8k.devImages.txt  \n",
            "  inflating: /content/Flickr8k_text/Flickr_8k.testImages.txt  \n",
            "  inflating: /content/Flickr8k_text/Flickr_8k.trainImages.txt  \n",
            "  inflating: /content/Flickr8k_text/readme.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Flickr8k_Dataset.zip -d /content/Flickr8k_Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eSHdiy_ZVsX",
        "outputId": "b14ae83b-3cd0-4b85-e52f-b15661329fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Flickr8k_Dataset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/Flickr8k_Dataset.zip or\n",
            "        /content/Flickr8k_Dataset.zip.zip, and cannot find /content/Flickr8k_Dataset.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TensorFlow\n",
        "!pip install Keras\n",
        "!pip install pillow\n",
        "!pip install NumPy\n",
        "#!Pip install jupyterlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OLmW8V4b-eS",
        "outputId": "59cc09d8-3ae9-4b94-8908-a0b728fa187d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: TensorFlow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (23.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (3.20.3)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (2.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (1.14.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (0.3.25)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (67.6.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (2.12.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (0.4.0)\n",
            "Collecting numpy<1.24,>=1.22\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (1.53.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (3.8.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (23.3.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (0.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from TensorFlow) (2.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->TensorFlow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->TensorFlow) (1.10.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->TensorFlow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->TensorFlow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->TensorFlow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->TensorFlow) (3.4.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->TensorFlow) (2.17.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->TensorFlow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->TensorFlow) (2.2.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->TensorFlow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->TensorFlow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->TensorFlow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->TensorFlow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->TensorFlow) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->TensorFlow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->TensorFlow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->TensorFlow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->TensorFlow) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->TensorFlow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->TensorFlow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->TensorFlow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->TensorFlow) (3.2.2)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.2\n",
            "    Uninstalling numpy-1.24.2:\n",
            "      Successfully uninstalled numpy-1.24.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax 0.1.6 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\n",
            "flax 0.6.8 requires jax>=0.4.2, but you have jax 0.3.25 which is incompatible.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (8.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: NumPy in /usr/local/lib/python3.9/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "83dd2cb6c23c4b81bdbc5b5895c3396e",
            "d36ec5f5727547ff9dbecda644c4e5c5",
            "6fd9e75862804e76bb62a752234e0884",
            "314ca4df6efd416cb68503858a5d8abc",
            "15bb9ded7ae3440b8f1a652c90c5d38b",
            "c3330e202d2349adafc05279591a2d5d",
            "7469b06907e14c7983783a67c6afb207",
            "575a78b13f244d0398530d7a11ad2200",
            "28d4dac9e4a1496d97760c89b00a7580",
            "3879e2bed47b4c6283d44ce16cafaf96",
            "3093af2fad3c489998a407cd73a23b96"
          ]
        },
        "id": "sx07qqcLsHzh",
        "outputId": "42d63230-74e4-47ea-b1d6-d98243f6dd7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-4922871b547d>:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  tqdm().pandas()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83dd2cb6c23c4b81bdbc5b5895c3396e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import string\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from keras.applications.xception import Xception #to get pre-trained model Xception\n",
        "from keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.text import Tokenizer #for text tokenization\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "#from keras.layers.merge import add\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense#Keras to build our CNN and LSTM\n",
        "from keras.layers import LSTM, Embedding, Dropout\n",
        "from tqdm import tqdm_notebook as tqdm #to check loop progress\n",
        "tqdm().pandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fp(filename):\n",
        "  # Open file to read\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "# get all images with their captions\n",
        "def img_capt(filename):\n",
        "    file = load_fp(filename)\n",
        "    captions = file.split('\\n')\n",
        "    descriptions ={}\n",
        "    for caption in captions[:-1]:\n",
        "        img, caption = caption.split('\\t')\n",
        "        if img[:-2] not in descriptions:\n",
        "            descriptions[img[:-2]] = [ caption ]\n",
        "        else:\n",
        "            descriptions[img[:-2]].append(caption)\n",
        "    return descriptions\n",
        "def txt_clean(captions):\n",
        "    table = str.maketrans('','',string.punctuation)\n",
        "    for img,caps in captions.items():\n",
        "        for i,img_caption in enumerate(caps):\n",
        "            img_caption.replace(\"-\",\" \")\n",
        "            descp = img_caption.split()\n",
        "          #uppercase to lowercase\n",
        "            descp = [wrd.lower() for wrd in descp]\n",
        "          #remove punctuation from each token\n",
        "            descp = [wrd.translate(table) for wrd in descp]\n",
        "          #remove hanging 's and a\n",
        "            descp = [wrd for wrd in descp if(len(wrd)>1)]\n",
        "          #remove words containing numbers with them\n",
        "            descp = [wrd for wrd in descp if(wrd.isalpha())]\n",
        "          #converting back to string\n",
        "            img_caption = ' '.join(descp)\n",
        "            captions[img][i]= img_caption\n",
        "    return captions"
      ],
      "metadata": {
        "id": "yfXCtvPgc8YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def txt_vocab(descriptions):\n",
        "  # To build vocab of all unique words\n",
        "    vocab = set()\n",
        "    for key in descriptions.keys():\n",
        "        [vocab.update(d.split()) for d in descriptions[key]]\n",
        "    return vocab\n",
        "#To save all descriptions in one file\n",
        "\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + '\\t' + desc )\n",
        "    data = \"\\n\".join(lines)\n",
        "    file = open(filename,\"w\")\n",
        "    file.write(data)\n",
        "    file.close()\n"
      ],
      "metadata": {
        "id": "OUUJP9spcEpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set these path according to project folder in you system, like i create a folder with my name shikha inside D-drive\n",
        "dataset_text = \"/content/Flickr8k_text\"\n",
        "dataset_images = \"Image Caption GeneratorFlicker8k_Dataset\"\n",
        "#to prepare our text data\n",
        "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
        "#loading the file that contains all data\n",
        "#map them into descriptions dictionary \n",
        "descriptions = img_capt(filename)\n",
        "print(\"Length of descriptions =\" ,len(descriptions))\n",
        "#cleaning the descriptions\n",
        "clean_descriptions = txt_clean(descriptions)\n",
        "#to build vocabulary\n",
        "vocabulary = txt_vocab(clean_descriptions)\n",
        "print(\"Length of vocabulary = \", len(vocabulary))\n",
        "#saving all descriptions in one file\n",
        "save_descriptions(clean_descriptions, \"descriptions.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfAw_PFb5iF_",
        "outputId": "f4e9416e-b823-4693-ec95-18eb2d3609cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of descriptions = 8092\n",
            "Length of vocabulary =  8763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = Xception( include_top=False, pooling='avg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH-IzxI4fN7i",
        "outputId": "4833ea5c-dada-41a3-f4a1-bbc8f5183569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(directory):\n",
        "    model = Xception( include_top=False, pooling='avg' )\n",
        "    features = {}\n",
        "    for pic in tqdm(os.listdir(dirc)):\n",
        "      file = dirc + \"/\" + pic\n",
        "      image = Image.open(file)\n",
        "      image = image.resize((299,299))\n",
        "      image = np.expand_dims(image, axis=0)\n",
        "      #image = preprocess_input(image)\n",
        "      image = image/127.5\n",
        "      image = image - 1.0\n",
        "      feature = model.predict(image)\n",
        "      features[img] = feature\n",
        "    return features"
      ],
      "metadata": {
        "id": "Lh75tgLiiQRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2048 feature vector\n",
        "features = extract_features(dataset_images)\n",
        "dump(features, open(\"features.p\",\"wb\"))\n",
        "#to directly load the features from the pickle file.\n",
        "features = load(open(\"features.p\",\"rb\"))"
      ],
      "metadata": {
        "id": "SlSK0zHij2Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data\n",
        "def load_photos(filename):\n",
        "    file = load_fp(filename)\n",
        "    photos = file.split(\"\\n\")[:-1]\n",
        "    return photos\n",
        "def load_clean_descriptions(filename, photos):\n",
        "  #loading clean_descriptions\n",
        "    file = load_fp(filename)\n",
        "    descriptions = {}\n",
        "    for line in file.split(\"\\n\"):\n",
        "        words = line.split()\n",
        "        if len(words)<1 :\n",
        "            continue\n",
        "        image, image_caption = words[0], words[1:]\n",
        "        if image in photos:\n",
        "            if image not in descriptions:\n",
        "                descriptions[image] = []\n",
        "                desc = ' ' + \" \".join(image_caption) + ' '\n",
        "                descriptions[image].append(desc)\n",
        "    return descriptions\n",
        "def load_features(photos):\n",
        "  #loading all feature\n",
        "    all_features = load(open(\"features.p\",\"rb\"))\n",
        "  #selecting only needed features\n",
        "    features = {k:all_features[k] for k in photos}\n",
        "    return features \n",
        "                    "
      ],
      "metadata": {
        "id": "LR-R3-P35CkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "#train = loading_data(filename)\n",
        "train_imgs = load_photos(filename)\n",
        "train_descriptions = load_clean_descriptions(\"/content/descriptions.txt\", train_imgs)\n",
        "train_features = load_features(train_imgs)"
      ],
      "metadata": {
        "id": "f-UlCyhh_g3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "umrZDyIi_hV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert dictionary to clear list of descriptions\n",
        "def dict_to_list(descriptions):\n",
        "    all_desc = []\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "#creating tokenizer class\n",
        "#this will vectorise text corpus\n",
        "#each integer will represent token in dictionary\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer\n",
        "# give each word an index, and store that into tokenizer.p pickle file\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "Vocab_size #The size of our vocabulary is 7577 words.\n",
        "#calculate maximum length of descriptions to decide the model structure parameters.\n",
        "def max_length(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    return max(len(d.split()) for d in desc_list)\n",
        "max_length = max_length(descriptions)\n",
        "Max_length #Max_length of description is 32"
      ],
      "metadata": {
        "id": "ntHbv98z56GU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "f9ac5901-fb31-4a3f-fc62-e44c2681d7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ea5746a3bad0>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# give each word an index, and store that into tokenizer.p pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_descriptions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data generator, used by model.fit_generator()\n",
        "def data_generator(descriptions, features, tokenizer, max_length):\n",
        "    while 1:\n",
        "      for key, description_list in descriptions.items():\n",
        "          #retrieve photo features\n",
        "          feature = features[key][0]\n",
        "          inp_image, inp_seq, op_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
        "          yield [[inp_image, inp_sequence], op_word]\n",
        "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
        "    x_1, x_2, y = list(), list(), list()\n",
        "  # move through each description for the image\n",
        "    for desc in desc_list:\n",
        "      # encode the sequence\n",
        "       seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "      # divide one sequence into various X,y pairs\n",
        "    for i in range(1, len(seq)):\n",
        "          # divide into input and output pair\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "          # pad input sequence\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "          # encode output sequence\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "          # store\n",
        "        x_1.append(feature)\n",
        "        x_2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "      return np.array(X_1), np.array(X_2), np.array(y)\n",
        "#To check the shape of the input and output for your model\n",
        "[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n",
        "a.shape, b.shape, c.shape\n",
        "#((47, 2048), (47, 32), (47, 7577))"
      ],
      "metadata": {
        "id": "XBePZpC26x4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}